{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T05:46:04.866645Z","iopub.execute_input":"2024-06-25T05:46:04.867653Z","iopub.status.idle":"2024-06-25T05:46:04.873150Z","shell.execute_reply.started":"2024-06-25T05:46:04.867612Z","shell.execute_reply":"2024-06-25T05:46:04.872222Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/styalai/pyHTM","metadata":{"execution":{"iopub.status.busy":"2024-06-25T05:46:06.116104Z","iopub.execute_input":"2024-06-25T05:46:06.116784Z","iopub.status.idle":"2024-06-25T05:46:22.074512Z","shell.execute_reply.started":"2024-06-25T05:46:06.116752Z","shell.execute_reply":"2024-06-25T05:46:22.073259Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/styalai/pyHTM\n  Cloning https://github.com/styalai/pyHTM to /tmp/pip-req-build-a97c0ro8\n  Running command git clone --filter=blob:none --quiet https://github.com/styalai/pyHTM /tmp/pip-req-build-a97c0ro8\n  Resolved https://github.com/styalai/pyHTM to commit 2bbbbead724b65279e44eee5e8daa00cb37ae411\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PyHTM\n  Building wheel for PyHTM (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyHTM: filename=PyHTM-0.1-py3-none-any.whl size=21608 sha256=46525263e642e3503a096e7494264dedea174996071734d84da3421bb3c87e87\n  Stored in directory: /tmp/pip-ephem-wheel-cache-iqvw9k6r/wheels/7d/d9/0e/d160669be1f9b0d84dcc802e2c016e91c4bddbd6fee3a8e9c8\nSuccessfully built PyHTM\nInstalling collected packages: PyHTM\nSuccessfully installed PyHTM-0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom math import *\nfrom pyhtm.model import HTM\nfrom pyhtm.pyhtm import ScalarEncoder, Regressor, SpatialPooler, TemporalMemory, plot_SDR","metadata":{"execution":{"iopub.status.busy":"2024-06-25T05:46:22.076731Z","iopub.execute_input":"2024-06-25T05:46:22.077066Z","iopub.status.idle":"2024-06-25T05:46:24.865041Z","shell.execute_reply.started":"2024-06-25T05:46:22.077032Z","shell.execute_reply":"2024-06-25T05:46:24.863833Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n  \"\"\"one head of self-attention\"\"\"\n\n  def __init__(self, head_size):\n      super().__init__()\n      self.key = nn.Linear(n_embd, head_size, bias=False)\n      self.query = nn.Linear(n_embd, head_size, bias=False)\n      self.value = nn.Linear(n_embd, head_size, bias=False)\n      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n      self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    T,C = x.shape\n    k = self.key(x)\n    q = self.query(x)\n    # compute attention score (\"affinities\")\n    wei = q @ k.transpose(-2, -1) * C**-0.5\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n    wei = F.softmax(wei, dim=-1)# (B, T, T)\n    wei = self.dropout(wei)\n    # perform the weighted aggregation of the values\n    v = self.value(x)\n    out = wei @ v # (B, T, C)\n    return out\n\n\n\n\nclass MultiHeadAttention(nn.Module):\n  \"\"\"multiple heads of self-attention in parallel\"\"\"\n\n  def __init__(self, num_heads, head_size):\n      super().__init__()\n      self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n      self.proj = nn.Linear(num_heads * head_size, n_embd)\n      self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    out = self.dropout(self.proj(out))\n    return out\n\n\n\n\nclass FeedForward(nn.Module):\n  \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n\n  def __init__(self, n_embd):\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(n_embd, 4 * n_embd),\n        nn.ReLU(),\n        nn.Linear(4 * n_embd, n_embd),\n        nn.Dropout(dropout),\n    )\n\n  def forward(self, x):\n    return self.net(x)\n\n\n\n\nclass Block(nn.Module):\n  \"\"\" Transformer block: communication followed by computation\"\"\"\n\n  def __init__(self, n_embd, n_head):\n     super().__init__()\n     head_size = n_embd // n_head\n     self.sa = MultiHeadAttention(n_head, head_size)\n     self.ffwd = FeedForward(n_embd)\n     self.ln1 = nn.LayerNorm(n_embd)\n     self.ln2 = nn.LayerNorm(n_embd)\n\n  def forward(self, x):\n    x = x + self.sa(self.ln1(x))\n    x = x + self.ffwd(self.ln2(x))\n    return x\n\n\nclass Transformer(nn.Module):\n\n    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, device):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.n_embd = n_embd\n        self.block_size = block_size\n        self.n_head = n_head\n        self.n_layer = n_layer\n        self.device = device\n\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(self.vocab_size, self.n_embd)\n        self.position_embedding_table = nn.Embedding(self.block_size, self.n_embd)\n        \n        inp_size = 100 * block_size\n        column_num = n_embd\n        active_cols = 4 * block_size\n        perm_inc = 0.04\n        perm_dec = 0.008\n        perm_thresh = 0.1\n        boost = 3\n        \n        self.encoder_fake = ScalarEncoder(n=inp_size, w=5, minval=0, maxval=vocab_size)\n        self.encoder = ScalarEncoder(n=100, w=5, minval=0, maxval=vocab_size)\n        \n        self.sp = SpatialPooler(\n                               source=self.encoder_fake, \n                               column_num=column_num,\n                               max_active_cols=active_cols,\n                               perm_increment=perm_inc,\n                               perm_decrement=perm_dec,\n                               boost_str=boost\n                               )\n        \n        self.blocks = nn.Sequential(*[Block(self.n_embd, n_head=self.n_head) for _ in range(self.n_layer)])\n        self.ln_f = nn.LayerNorm(self.n_embd)\n        self.lm_head = nn.Linear(self.n_embd, self.vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        inp = idx[0].tolist()\n        sdrs = []\n        for i in inp:\n            sdrs.append(torch.tensor(self.encoder.encode(i)))\n        sdr = torch.cat(sdrs).numpy()\n        \n        out = self.sp.process_input(sdr)\n        \n        out_htm = torch.from_numpy(out).unsqueeze(0)\n        \n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # T, C\n\n        x = tok_emb + pos_emb # (B, T, C)\n        x = x.squeeze(0)\n        x = out_htm.float()\n\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x) # (B, T, self.vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            T, C = logits.shape\n            logits = logits.view(T, C)\n            targets = targets.view(T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last self.block_size tokens\n            idx_cond = idx[:, -self.block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx, idx_next\n\n\n\n@torch.no_grad()\ndef estimate_loss(model, eval_iters, block_size, batch_size):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split, block_size, batch_size)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T05:46:24.866844Z","iopub.execute_input":"2024-06-25T05:46:24.867838Z","iopub.status.idle":"2024-06-25T05:46:24.910586Z","shell.execute_reply.started":"2024-06-25T05:46:24.867798Z","shell.execute_reply":"2024-06-25T05:46:24.909417Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"try :\n  with open('input.txt', 'r', encoding='utf-8') as f:\n      text = f.read()\nexcept:\n  !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n  with open('input.txt', 'r', encoding='utf-8') as f:\n      text = f.read()\nprint(len(text))\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.1*len(data)) # first 90% will be train, rest val\ntrain_data = data[n:]\nval_data = data[:n]\n\n\"\"\"\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\"\"\"\n\n# data loading\ndef get_batch(split, block_size, batch_size):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-06-25T05:46:24.913144Z","iopub.execute_input":"2024-06-25T05:46:24.913921Z","iopub.status.idle":"2024-06-25T05:46:26.409094Z","shell.execute_reply.started":"2024-06-25T05:46:24.913862Z","shell.execute_reply":"2024-06-25T05:46:26.407979Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"--2024-06-25 05:46:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n\n2024-06-25 05:46:26 (47.1 MB/s) - 'input.txt' saved [1115394/1115394]\n\n1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 1 # how many independent sequences will we process in parallel?\nblock_size = 20 # what is the maximum context length for predictions? # impact little\neval_iters = 3 # more fast ( when it's low )\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nn_embd = 154  # impact big 8*12\nn_head = 6    # no impact\nn_layer = 4   # impact\ndropout = 0.2 # no impact\n# ------------\n\nmodel = Transformer(vocab_size, n_embd, block_size, n_head, n_layer, device)\nm = model.to(device)\n#torch.save(m.state_dict(), '/content/drive/MyDrive/Colab Notebooks/models/hound7,1M')\n\nparas = list(str(sum(p.numel() for p in m.parameters())))\nnum = len(paras)-1\nfor i in paras:\n  if num % 3 == 0:\n    print(i, end=\" \")\n    pass\n  else:\n    print(i, end=\"\")\n    pass\n  num -= 1","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:06:11.924077Z","iopub.execute_input":"2024-06-24T19:06:11.924439Z","iopub.status.idle":"2024-06-24T19:06:11.985866Z","shell.execute_reply.started":"2024-06-24T19:06:11.924410Z","shell.execute_reply":"2024-06-24T19:06:11.984833Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"1 158 145 ","output_type":"stream"}]},{"cell_type":"code","source":"model = Transformer(vocab_size, n_embd, block_size, n_head, n_layer, device)\n#model.load_state_dict(torch.load('/content/modelchars1'))\nm = model.to(device)\n\n# create a PyTorch optimizer\nlearning_rate = 3e-4\noptimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\nloss_list_t = []\nloss_list_v = []\nmax_iters = 1000\neval_interval = 100\n\nfor iter in tqdm(range(max_iters)):\n\n    # every once in a while evaluate the loss on train and val sets\n\n    if iter % eval_interval == 0:\n        #torch.save(m.state_dict(), '/content/modelchars1')\n\n        losses = estimate_loss(m, eval_iters, block_size, batch_size)\n        loss_list_t.append(losses['train'])\n        loss_list_v.append(losses['val'])\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n\n    # sample a batch of data\n    xb, yb = get_batch('train', block_size, batch_size)\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    #if iter % eval_interval == 0:\n     # loss_list.append(loss.item())\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# draw loss\nprint(loss)\nplt.plot(range(len(loss_list_t)), loss_list_t)\nplt.plot(range(len(loss_list_v)), loss_list_v)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:07:15.824051Z","iopub.execute_input":"2024-06-24T19:07:15.824513Z","iopub.status.idle":"2024-06-24T19:07:16.091201Z","shell.execute_reply.started":"2024-06-24T19:07:15.824480Z","shell.execute_reply":"2024-06-24T19:07:16.089492Z"},"trusted":true},"execution_count":92,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304dfe01308a4dd5a4061bac946cc1d6"}},"metadata":{}},{"name":"stdout","text":"torch.Size([1, 65])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[92], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(max_iters)):\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m#torch.save(m.state_dict(), '/content/modelchars1')\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         loss_list_t\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m         loss_list_v\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[87], line 183\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model, eval_iters, block_size, batch_size)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m    182\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split, block_size, batch_size)\n\u001b[0;32m--> 183\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    185\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[87], line 151\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    149\u001b[0m     T, C \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    150\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(T, C)\n\u001b[0;32m--> 151\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[1]' is invalid for input of size 20"],"ename":"RuntimeError","evalue":"shape '[1]' is invalid for input of size 20","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}